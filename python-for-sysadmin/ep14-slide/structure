
- Intro to ipython 15m
- Gathering system data 30m
- NoseTesting 15m
- Parsing 60m
- Processing data 45m
- Plotting 15m


# Intro to ipython 15m
## Goal
Review of ipython features and some useful functions. Class skill assessment.
## Agenda
  - tab completion
  - running commands and filtering output
  - reading text files
  - modules: __builtin__, ipython
## Exercise
 - running commands in ipython

        > # on unix
        > ret = ! cat /etc/hosts
        > # windows has etc\hosts too ;)
        > ret = !type c:\windows\system32\drivers\etc\hosts

 - ipython grep and fields

        > ret.grep('localhost')
        > ret.fields(0)
        > ret.grep('localhost').fields(0)
 
- a simple grep implementation

         > def grep(needle, fpath):
         >     return [x for x in open(fpath) if needle in x]
 

# Gathering system data 30m 
## Goal
Gather system data from the operating system and system commands into list and dict. 

## Agenda
- multiplatform filename management: the os.path module
- shell globbing and expansion
- getting command output with subprocess.check_output
- use external module psutil for multiplatform data gathering
- modules: os, glob, subprocess, time, psutil
## Labs and Exercises
- simple and multiplatform path management

        > hosts, basedir = "etc/hosts", "/"
        > if 'win' in sys.platform:
        >     basedir = 'c:/windows/system32/drivers'
        > hosts = os.path.join(basedir, hosts)
        > hosts = os.path.normpath(hosts)
        > print("Normalized path is", hosts)

- Show threads information using glob and list comprehensions (Linux)

        > import glob
        > def linux_threads(pid):
        >     path = "/proc/{}/task/*/status".format(pid)
        >     t_info = ('Pid', 'Tgid', 'voluntary')
        >     for t in glob.glob(path):
        >         t_info = [x for x in open(t) if x.startswith(t_info) ]
        >         print(t_info)    

- Invoking command output

        > from subprocess import check_output
        > def sh(cmd, shell=True):
        >     output = check_output(cmd.split(), shell=shell)
        >     return output.splitlines()

- Using psutils for multiplatform data gathering

        > from psutil import phymem_usage,  cpu_percent, disk_io_counter
        > 

- Implement a pgrep-like function using `grep` and `sh`;
- Write a multiplatform iostat.py using psutil; 


# NoseTesting 15m
## Goal
Run a simple test script with setup and teardown. 
## Agenda
 - importance of testing
 - nose testing framework
 - a sample script 
 - capturing output and logging  
## Exercise
- a simple nosetest script

        > def setup():
        >    print("before the testsuite")
        > def teardown():
        >    print("after the testsuite")
        > def test_one():
        >     assert os.path.isfile("/etc/hosts"), "Missing /etc/hosts"

- a [complex test template on github](https://github.com/ioggstream/python-course/blob/master/python-for-sysadmin/02_nosetests_full.py)

- write a script with some assertion on our PC hardware (disk size, number of cpus, free virtual memory) using psutil. Use tab-completion or help(psutil) to discover all its features.

# Parsing 60m
## Goal
Parse simple log files and estimate parsing time.
## Agenda 
 - parsing best practices
    - create a parse_test function
    - write parsing code
    - create a library containing test
    - run simple benchmarks on parsing code with %timeit
  - basic regular expressions
    - re.findall, re.split
    - advantages of re.compile (readability, performances)
### Real life example
A rapid glance to a real-life example:  [py-jstack code is a Tomcat Heap Monitor](https://github.com/ioggstream/py-jstack)

## Exercise
- test driven parsing

        > testline = "May 31 08:30:55 test-fe1 postfix/smtp[16669]: 7CD8E730020: removed"
        > def test_one():
        >    ret = parse_maillog(testline)
        >    assert ret == ('08:30:55', 'test-fe1', '7CD8E730020'), "Error %r" % ret

- write the parse_maillog function, bench with %timeit

- grep reloaded with regular expressions and path normalization

        > import re
        > def grep(expr, fpath):
        >     re_expr = re.compile(expr)
        >     fpath = os.path.normpath(fpath)
        >     with open(fpath) as fp:
        >         return [ x for x in fp if re_expr.match(x) ]
        >

- splitting with re.split

        >
        >    cmd = "ping -c10 -w10 www.google.it"
        >    # on windows "ping -n10 www.google.it" 
        >    ping_output = sh(cmd)
        >    ping_output = [re.split("[ =", x) for x in ping_output]
        >

- splitting hex digits by ":" using re.findall 

        > re_s_1 = "[0-9a-fA-F]"
        > mac = "0024e8b43320"
        > mac = ':'.join(re.findall(re_s_1, mac))

- time the above exercise replacing re_s_1 with the followings

        > test_all_regexps = ("..", "[a-f0-9]{2}")
        > for re_s in test_all_regexps:
        >     %timeit ':'.join(re.findall(re_s, mac))

- time the above using re.compile

        > re_1 = re.compile(re_s_1)
        > mac = ':'.join(re_1.findall(mac))

- create a storage configuration script 

        >  fc_id_path = "/sys/class/fc_host/host*/port_name"
        >  for x in glob.glob(fc_id_path):
        >      pwwn = open(x).read() #  0x500143802427e66c
        >      pwwn = pwwn[2:]
        >      pwwn = re.findall(r'..', pwwn)
        >      print("member pwwn ", ':'.join(pwwn) )

- parse the [provided maillog file](https://github.com/ioggstream/python-course/blob/master/python-for-sysadmin/) and get all destination domains. You can use the [maillog test template](https://github.com/ioggstream/python-course/blob/master/python-for-sysadmin/03_parsing_test.py)


# Simple Processing 45m
## Goal
Manage the gathered data and find relation between them. Get essential information like standard deviation, distribution and linear correlation. 

## Agenda
 - the zip function, transposing data with zip
- basic statistics with scipy
   - max, min and standard deviation: the importance of stdev
   - data distribution and percentiles
   - linear correlation: what's linear correlation and when can help
- modules: scipy, collections

## Exercise
- use zip to transpose previous ping_output

       > ping_output  = [ping_output[-6:-1:2] for x in ping_output]
       > \# on windows ping_output[4::2]
       > ping_no, ttl, ping_rtt = zip(*ping_output)

- use scipy to find basic stats from ping_output
       
        > ping_rtt = map(float, ping_rtt) 
        > from scipy import std, mean
        > fmt_s = 'stdev: {}, mean: {}, min: {}, max: {}'
        > rtt_std, rtt_mean = std(ping_rtt), mean(ping_rtt)
        > rtt_max, rtt_min = max(ping_rtt), min(ping_rtt)
        > print(fmt_s.format(rtt_std,rtt_mean,rtt_max,rtt_min))

- generate data distribution using set or defaultdict

    > distro = { x:  ping_rtt.count(x) for x in set(ping_rtt)}
    > \# or
    > distro = collections.defaultdict(int)
    > for x in ping_rtt: distro += 1

- parse [the provided  maillog](https://github.com/ioggstream/python-course/tree/master/python-for-sysadmin/data) and get the 4-hourly email distribution

- use the system_monitor() function to gather system data under load and find linear correlation between cpu% and I/O usage. 

       > from scipy.stats.stats import pearsonr 
       > r, p = pearsonr(cpu_usage, io_usage)

# Plotting 15m
## Goal
 Understand what and howto plot and infer system behavior from plot.
## Agenda
 - matplotlib
 - use scatter plots to show relations
 - use histograms to find data and time distribution
 - using colors to mark time in 2d plot 
## Exercise
- a simple scatter plot
        > from matplotlib import pyplot as plt
        > X, Y = range(100), range(0,200,2)
        > plt.scatter(X, Y)
- use scatter plot to relate the previously collected cpu_percent and io_usage 
- an histogram plot
         > X = [random.randint(10) for x in range (100)]
         > distribution, percentiles, _ = plt.hist(X)
         > plt.show()

- scatter and histogram plotting data from previous  exercises:
    - eg. maillog hourly distribution


# Bonus Tracks
If the class finish before the 3 hrs (eg. people is acquainted with python) I usually let people choiche between:  
 - review exercises and ask job-related questions;
 - show how to use PSL to gather data;
 - show how to expose monitoring data with flask

## PSL Toolkit  

### Goal
Using programs embedded in the PSL (telnet, http, smtp and imap client) for simple monitoring
### Agenda
- telnetlib
- smtplib
- imaplib 
- SimpleHTTPServer

### Exercise
- to be written in case of selection ;)

## Flask
Using flask to serve monitored data
### Agenda
- the flask microframework
- a simple test page
- expose monitoring data via web
### Exercise
 - to be written in case of selection ;)
